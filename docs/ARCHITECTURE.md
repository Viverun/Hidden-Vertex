# Architecture Guide - The Hidden Vertex

> **Technical deep dive into the Graph Autoencoder architecture for unsupervised particle physics anomaly detection**

## Table of Contents
- [Overview](#overview)
- [Why Graph Neural Networks?](#why-graph-neural-networks)
- [Model Architecture](#model-architecture)
- [Mathematical Formulation](#mathematical-formulation)
- [Design Decisions](#design-decisions)
- [Loss Functions](#loss-functions)
- [Theoretical Foundations](#theoretical-foundations)

---

## Overview

The Hidden Vertex uses a **Graph Autoencoder (GAE)** to learn the manifold of Standard Model physics in an unsupervised manner. By compressing particle collision events through a narrow 10-dimensional bottleneck, the model is forced to learn the fundamental physical laws governing particle interactions.

### High-Level Architecture

```
Input: Particle Graph (100-300 nodes, 3 features each)
                    ‚Üì
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ   ENCODER (GNN)      ‚îÇ
         ‚îÇ  3 ‚Üí 64 ‚Üí 32 ‚Üí 10    ‚îÇ  ‚Üê Compression forces learning
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
         [10D Latent Vector]      ‚Üê Physics manifold
                    ‚Üì
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ   DECODER (MLP)      ‚îÇ
         ‚îÇ  10 ‚Üí 64 ‚Üí 3         ‚îÇ  ‚Üê Reconstruction
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
  Reconstructed Graph (pT, Œ∑, œÜ)
                    ‚Üì
         [Reconstruction Error]
                    ‚Üì
  Low error = Standard Model ‚úÖ
  High error = Anomaly! üö®
```

---

## Why Graph Neural Networks?

### The Problem with CNNs

Traditional approaches in particle physics use **Convolutional Neural Networks (CNNs)**, treating detector data as images:

```
Particle Data (Sparse)  ‚Üí  Image Grid (Dense)  ‚Üí  CNN
   [10-300 particles]        [64√ó64 pixels]
```

**Problems:**
1. **Sparsity waste:** Most pixels are empty (0 occupancy)
2. **Information loss:** Precise (Œ∑, œÜ) coordinates ‚Üí grid quantization
3. **Fixed structure:** CNNs expect regular grids, particles form irregular graphs
4. **Computational waste:** 99% of operations on zeros

### The Graph Approach

**Particles are naturally graphs!**

```
Particles = Nodes (features: pT, Œ∑, œÜ)
   ‚Üì
Build edges via k-NN in (Œ∑, œÜ) space
   ‚Üì
Result: Graph G = (V, E, X)
```

**Advantages:**
1. **Sparsity-aware:** Only process actual particles
2. **Permutation invariant:** Order doesn't matter (like real physics!)
3. **Geometric:** Preserves precise spatial relationships
4. **Efficient:** ~100 nodes vs ~4096 pixels

---

## Model Architecture

### Full Architecture Diagram

```
Input Graph G = (V, E, X)
  V: Nodes (particles)
  E: Edges (k-NN relationships)  
  X: Node features [N, 3]
        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ        ENCODER (GNN)            ‚îÇ
‚îÇ                                 ‚îÇ
‚îÇ  GCNConv Layer 1: 3 ‚Üí 64        ‚îÇ ‚Üê Message passing
‚îÇ    ‚Ä¢ Aggregates neighbor info  ‚îÇ
‚îÇ    ‚Ä¢ ReLU activation            ‚îÇ
‚îÇ    ‚Ä¢ Dropout (0.1)              ‚îÇ
‚îÇ                                 ‚îÇ
‚îÇ  GCNConv Layer 2: 64 ‚Üí 32       ‚îÇ ‚Üê Higher-level features
‚îÇ    ‚Ä¢ Global jet structure       ‚îÇ
‚îÇ    ‚Ä¢ ReLU activation            ‚îÇ
‚îÇ    ‚Ä¢ Dropout (0.1)              ‚îÇ
‚îÇ                                 ‚îÇ
‚îÇ  Global Mean Pooling            ‚îÇ ‚Üê Graph-level embedding
‚îÇ    [N, 32] ‚Üí [1, 32]            ‚îÇ
‚îÇ                                 ‚îÇ
‚îÇ  MLP Encoder: 32 ‚Üí 10           ‚îÇ ‚Üê Bottleneck compression
‚îÇ    ‚Ä¢ Linear + ReLU + Dropout    ‚îÇ
‚îÇ    ‚Ä¢ Linear ‚Üí latent vector     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì
    [z ‚àà ‚Ñù¬π‚Å∞]  ‚Üê Latent space (physics manifold)
        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ        DECODER (MLP)            ‚îÇ
‚îÇ                                 ‚îÇ
‚îÇ  MLP Layer 1: 10 ‚Üí 32           ‚îÇ ‚Üê Expand latent
‚îÇ    ‚Ä¢ Linear + ReLU + Dropout    ‚îÇ
‚îÇ                                 ‚îÇ
‚îÇ  MLP Layer 2: 32 ‚Üí 64           ‚îÇ ‚Üê Intermediate
‚îÇ    ‚Ä¢ Linear + ReLU              ‚îÇ
‚îÇ                                 ‚îÇ
‚îÇ  Broadcasting                   ‚îÇ ‚Üê Replicate to all nodes
‚îÇ    [1, 64] ‚Üí [N, 64]            ‚îÇ
‚îÇ                                 ‚îÇ
‚îÇ  Node Reconstruction: 64 ‚Üí 3    ‚îÇ ‚Üê Predict (pT, Œ∑, œÜ)
‚îÇ    ‚Ä¢ Linear (no activation)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì
    XÃÇ ‚àà ‚Ñù·¥∫À£¬≥  ‚Üê Reconstructed features
        ‚Üì
    Loss = MSE(X, XÃÇ)
```

### Encoder: Graph ‚Üí Latent

#### 1. Graph Convolution Layers

**GCNConv (Graph Convolutional Network):**

```python
class Encoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = GCNConv(3, 64)
        self.conv2 = GCNConv(64, 32)
        self.fc_encode = nn.Sequential(
            nn.Linear(32, 32),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(32, 10)
        )
    
    def forward(self, x, edge_index, batch):
        # Layer 1: Learn local jet structure
        h = F.relu(self.conv1(x, edge_index))
        h = F.dropout(h, p=0.1, training=self.training)
        
        # Layer 2: Learn global momentum conservation
        h = F.relu(self.conv2(h, edge_index))
        h = F.dropout(h, p=0.1, training=self.training)
        
        # Global pooling: graph ‚Üí vector
        h_graph = global_mean_pool(h, batch)  # [batch_size, 32]
        
        # Compress to latent space
        z = self.fc_encode(h_graph)  # [batch_size, 10]
        
        return z
```

**What GCNConv Does:**

For each node i:
```
h·µ¢‚ÅΩÀ°‚Å∫¬π‚Åæ = œÉ(Œ£‚±º‚ààN(i) (1/‚àö(d·µ¢d‚±º)) ¬∑ W‚ÅΩÀ°‚Åæ ¬∑ h‚±º‚ÅΩÀ°‚Åæ)
```

Where:
- N(i): Neighbors of node i (via k-NN edges)
- d·µ¢, d‚±º: Node degrees (normalization)
- W‚ÅΩÀ°‚Åæ: Learnable weight matrix
- œÉ: Activation function (ReLU)

**Physical Interpretation:**
- Layer 1: Learns relationships between nearby particles (jet substructure)
- Layer 2: Learns global event topology (momentum balance)

#### 2. Global Pooling

Converts variable-size graphs ‚Üí fixed-size vectors:

```python
h_graph = global_mean_pool(h, batch)
```

**Mean Pooling:**
```
h_graph = (1/N) Œ£·µ¢‚Çå‚ÇÅ·¥∫ h·µ¢
```

**Alternatives:**
- `global_max_pool`: Takes maximum activation
- `global_add_pool`: Sums activations
- Attention pooling: Weighted sum

**Why Mean?** Provides robust summary, invariant to particle ordering.

#### 3. Latent Bottleneck

**The Critical Compression:**

```python
z = self.fc_encode(h_graph)  # 32 ‚Üí 10 dimensions
```

**Why 10 Dimensions?**

This is the **information bottleneck** that forces learning:

- **Too large (e.g., 100D):** Model memorizes, no generalization
- **Too small (e.g., 2D):** Cannot capture physics complexity
- **Just right (10D):** Forces model to learn:
  - Conservation laws (energy, momentum)
  - Jet topology patterns
  - Radiation structure
  - Fundamental symmetries

**Latent Space = Physics Manifold**

Background events cluster tightly in 10D space:
```
Standard Model ‚Üí Smooth manifold in ‚Ñù¬π‚Å∞
New Physics ‚Üí Off-manifold ‚Üí Cannot reconstruct
```

### Decoder: Latent ‚Üí Graph

#### 1. Expansion

```python
class Decoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc_decode = nn.Sequential(
            nn.Linear(10, 32),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(32, 64),
            nn.ReLU()
        )
        self.node_reconstructor = nn.Linear(64, 3)
    
    def forward(self, z, batch):
        # Expand latent code
        h = self.fc_decode(z)  # [batch_size, 64]
        
        # Broadcast to all nodes in each graph
        h_broadcast = h[batch]  # [total_nodes, 64]
        
        # Reconstruct node features
        x_recon = self.node_reconstructor(h_broadcast)  # [total_nodes, 3]
        
        return x_recon
```

#### 2. Broadcasting

**Key Challenge:** Latent vector is per-graph, but we need per-node predictions.

**Solution:** Replicate graph embedding to all nodes:

```python
# z: [batch_size, 10]
# batch: [total_nodes] - indicates which graph each node belongs to
# h[batch]: [total_nodes, 64] - broadcasts graph embedding to nodes
```

**Example:**
```
Graph 0: nodes [0, 1, 2]     ‚Üí all get h[0]
Graph 1: nodes [3, 4, 5, 6]  ‚Üí all get h[1]
```

#### 3. Node Feature Reconstruction

```python
x_recon = self.node_reconstructor(h_broadcast)
```

Predicts (pT, Œ∑, œÜ) for each particle:
- **No activation** on output (regression task)
- **Normalized features** (log(pT), Œ∑, œÜ)

---

## Mathematical Formulation

### Complete Forward Pass

Given input graph **G = (V, E, X)**:

**Encoding:**
```
H‚ÅΩ¬π‚Åæ = ReLU(GCN(X, E))           ‚àà ‚Ñù·¥∫À£‚Å∂‚Å¥
H‚ÅΩ¬≤‚Åæ = ReLU(GCN(H‚ÅΩ¬π‚Åæ, E))        ‚àà ‚Ñù·¥∫À£¬≥¬≤
h_graph = (1/N) Œ£·µ¢ H‚ÅΩ¬≤‚Åæ·µ¢          ‚àà ‚Ñù¬≥¬≤
z = MLP(h_graph)                  ‚àà ‚Ñù¬π‚Å∞
```

**Decoding:**
```
h_decoded = MLP(z)                ‚àà ‚Ñù‚Å∂‚Å¥
H_broadcast = h_decoded[batch]    ‚àà ‚Ñù·¥∫À£‚Å∂‚Å¥
XÃÇ = Linear(H_broadcast)           ‚àà ‚Ñù·¥∫À£¬≥
```

**Loss:**
```
‚Ñí = MSE(X, XÃÇ) = (1/N) Œ£·µ¢ ||X·µ¢ - XÃÇ·µ¢||¬≤
```

### Anomaly Score

For a test graph G:
```
z = Encoder(G)
XÃÇ = Decoder(z)
score = ||X - XÃÇ||¬≤ / N

if score > threshold:
    ‚Üí Anomaly detected! üö®
else:
    ‚Üí Standard Model ‚úÖ
```

---

## Design Decisions

### 1. Why k-NN Graph Construction?

**Approach:** Build edges between k nearest neighbors in (Œ∑, œÜ) space.

```python
edge_index = knn_graph(x[:, 1:3], k=6, loop=False)
```

**Rationale:**
- **Physics-informed:** Particles close in detector space are causally related
- **k=6:** Captures immediate neighborhood without over-connecting
- **No self-loops:** Avoids trivial message passing

**Alternatives Considered:**
- **Radius graph:** Variable connectivity (unstable)
- **Fully connected:** O(N¬≤) edges (expensive, noisy)
- **Distance-weighted:** Adds complexity without clear benefit

### 2. Why Global Mean Pooling?

**Options:**
```python
# Mean (chosen)
h = global_mean_pool(h_node, batch)

# Max
h = global_max_pool(h_node, batch)

# Sum
h = global_add_pool(h_node, batch)
```

**Why Mean?**
- **Size invariant:** Works for 10 or 300 particles
- **Robust:** Less sensitive to outliers than max
- **Physics:** Represents "average" event characteristics

### 3. Why 10D Latent Space?

**Empirical Testing:**

| Dimensions | Reconstruction | Separation | Training |
|------------|---------------|------------|----------|
| 2D | Poor | Good | Fast |
| 5D | Fair | Good | Fast |
| **10D** | **Good** | **Best** | **Fast** |
| 20D | Good | Moderate | Medium |
| 50D | Excellent | Poor | Slow |

**10D is the sweet spot:**
- Enough capacity to encode physics
- Small enough to force generalization
- Fast training convergence

### 4. Architecture Depth

**Why 2 GCN Layers?**

Tested 1-4 layers:
```
1 layer:  Local features only, poor performance
2 layers: ‚úÖ Captures local + global structure
3 layers: Over-smoothing, diminishing returns
4 layers: Severe over-smoothing
```

**Over-smoothing:** Node features become too similar after many layers of aggregation.

### 5. Dropout Rate

**p = 0.1** chosen empirically:
- **p = 0:** Overfitting on training data
- **p = 0.1:** ‚úÖ Good generalization
- **p = 0.3:** Underfitting, poor reconstruction

---

## Loss Functions

### Mean Squared Error (MSE)

**Current implementation:**

```python
loss = F.mse_loss(x_reconstructed, x_original)
```

**Formula:**
```
‚Ñí_MSE = (1/N) Œ£·µ¢‚Çå‚ÇÅ·¥∫ Œ£‚±º‚Çå‚ÇÅ¬≥ (x·µ¢‚±º - xÃÇ·µ¢‚±º)¬≤
```

**Pros:**
- Simple, differentiable
- Works well with our graph structure
- Fast to compute

**Cons:**
- Assumes fixed particle ordering
- Sensitive to outliers

### Chamfer Distance (Advanced)

**For permutation invariance:**

```python
def chamfer_distance(x, x_hat):
    """
    Permutation-invariant point cloud distance
    """
    # Distance matrix: [N, N]
    dist_matrix = torch.cdist(x, x_hat)
    
    # For each original particle, find nearest reconstruction
    min_orig = dist_matrix.min(dim=1)[0].mean()
    
    # For each reconstructed particle, find nearest original
    min_recon = dist_matrix.min(dim=0)[0].mean()
    
    # Symmetric Chamfer distance
    return min_orig + min_recon
```

**Formula:**
```
‚Ñí_CD = (1/N) Œ£·µ¢ min_‚±º ||x·µ¢ - xÃÇ‚±º||¬≤ + (1/N) Œ£‚±º min_·µ¢ ||x·µ¢ - xÃÇ‚±º||¬≤
```

**When to use:**
- Particles are truly unordered
- Need strict permutation invariance
- Have computational budget (O(N¬≤))

**Trade-off:** MSE works well in practice, Chamfer is more principled.

---

## Theoretical Foundations

### Manifold Learning Perspective

**Hypothesis:** Standard Model events lie on a low-dimensional manifold M ‚äÇ ‚Ñù·¥∫À£¬≥.

**Autoencoder Goal:** Learn mapping that:
1. Projects onto manifold: **Encoder(x) ‚àà M**
2. Reconstructs from manifold: **Decoder(z) ‚âà x** if **x ‚àà M**

**Anomaly Detection:**
```
If x ‚àà M:     Reconstruction succeeds (low error)
If x ‚àâ M:     Projection fails (high error)
```

### Information Bottleneck Theory

The 10D bottleneck enforces:
```
I(Z; X) ‚â§ 10 bits  (limited information flow)
```

**Result:** Model must compress X to its most informative features:
- Conservation laws
- Symmetries  
- Typical topologies

**Irrelevant details discarded:**
- Noise
- Pile-up
- Random fluctuations

### Comparison to Other Architectures

**vs. Variational Autoencoder (VAE):**
```
VAE: p(z|x) is Gaussian, samples from distribution
GAE: z = f(x) is deterministic, learns manifold directly
```
**Why GAE?** Simpler, no KL divergence tuning, works well for anomaly detection.

**vs. CNN Autoencoder:**
```
CNN: X ‚Üí Image ‚Üí CNN ‚Üí Latent ‚Üí CNN ‚Üí Image ‚Üí X
GAE: X ‚Üí Graph ‚Üí GNN ‚Üí Latent ‚Üí MLP ‚Üí Graph ‚Üí X
```
**Why GAE?** Sparsity-aware, preserves geometry, permutation invariant.

**vs. Transformer:**
```
Transformer: Attention over all particles (O(N¬≤))
GNN: Message passing over edges (O(E))
```
**Why GNN?** More efficient, physics-informed structure.

---

## Implementation Details

### Input Normalization

**Critical for training stability:**

```python
# Raw features (WRONG - causes loss = 1500+)
x = [pT, Œ∑, œÜ]  # pT: 0.1-850, Œ∑: -5-5, œÜ: -œÄ-œÄ

# Normalized features (CORRECT)
x_norm = [
    torch.log(pT + 1e-8),  # log(pT): -2 to 7
    Œ∑,                      # Œ∑: -5 to 5 (already good)
    œÜ                       # œÜ: -œÄ to œÄ (already good)
]
```

**Why log(pT)?**
- pT has exponential distribution
- Log transform ‚Üí roughly Gaussian
- All features on similar scale

### Edge Construction

```python
# Build k-NN graph in (Œ∑, œÜ) space
coords = x[:, 1:3]  # [Œ∑, œÜ]
edge_index = knn_graph(coords, k=6, loop=False)
```

**Result:**
- ~6 edges per node
- Sparse connectivity
- Physics-informed structure

### Batching Strategy

PyTorch Geometric batches by **concatenation**:

```python
# Batch of 3 graphs
Graph 0: 100 nodes ‚Üí indices 0-99
Graph 1: 150 nodes ‚Üí indices 100-249  
Graph 2: 80 nodes  ‚Üí indices 250-329

# Batch tensor tracks assignment
batch = [0,0,...,0, 1,1,...,1, 2,2,...,2]
        ‚îî‚îÄ 100x ‚îÄ‚îÄ‚îò ‚îî‚îÄ 150x ‚îÄ‚îÄ‚îò ‚îî‚îÄ 80x ‚îÄ‚îò
```

**Advantages:**
- Efficient GPU operations
- No padding needed
- Handles variable sizes naturally

---

## Extensions and Future Work

### 1. Attention Mechanisms

Add attention to capture importance:

```python
class AttentionPooling(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.attention = nn.Linear(hidden_dim, 1)
    
    def forward(self, h, batch):
        # Compute attention weights
        weights = F.softmax(self.attention(h), dim=0)
        
        # Weighted sum per graph
        h_graph = scatter_add(weights * h, batch, dim=0)
        return h_graph
```

### 2. Hierarchical Encoding

Multi-scale latent space:

```python
# Different resolutions
z_coarse = encoder_coarse(x)   # 5D - global topology
z_fine = encoder_fine(x)       # 20D - detailed features

# Combine for reconstruction
x_recon = decoder([z_coarse, z_fine])
```

### 3. Contrastive Learning

Learn representations via similarity:

```python
# Pull similar events together
# Push dissimilar events apart
loss = contrastive_loss(z_anchor, z_positive, z_negative)
```

### 4. Graph Generation

Fully generative model:

```python
# Sample from latent space
z ~ N(0, I)

# Decode to graph
x, edges = decoder(z)

# Generate new physics events!
```

---

## Hyperparameter Summary

**Model Architecture:**
```yaml
encoder:
  conv1: 3 ‚Üí 64
  conv2: 64 ‚Üí 32
  latent: 32 ‚Üí 10

decoder:
  expand: 10 ‚Üí 32 ‚Üí 64
  reconstruct: 64 ‚Üí 3

pooling: global_mean
dropout: 0.1
```

**Training:**
```yaml
optimizer: AdamW
learning_rate: 1e-3
weight_decay: 1e-5
batch_size: 32
max_epochs: 50
early_stopping: 10
gradient_clip: 1.0
```

**Data:**
```yaml
graph_construction: knn
k_neighbors: 6
feature_norm: log(pT)
train_fraction: 0.7
val_fraction: 0.15
test_fraction: 0.15
```

---

## References

**Graph Neural Networks:**
- Kipf & Welling (2017): "Semi-Supervised Classification with Graph Convolutional Networks"
- Fey & Lenssen (2019): "Fast Graph Representation Learning with PyTorch Geometric"

**Autoencoders for Anomaly Detection:**
- Goodfellow et al. (2016): "Deep Learning" (Chapter 14)
- Chalapathy & Chawla (2019): "Deep Learning for Anomaly Detection"

**Particle Physics Applications:**
- Komiske et al. (2019): "Energy Flow Networks"
- Qu & Gouskos (2020): "ParticleNet"
- Kasieczka et al. (2021): "The LHC Olympics 2020"

---

## Code Reference

See implementation in:
- `src/model/autoencoder.py` - Main model
- `src/model/layers.py` - Custom GNN layers
- `src/data/preprocessing.py` - Graph construction
- `src/training/losses.py` - Loss functions

---

**Questions?** Open an issue or see [TRAINING.md](TRAINING.md) for practical usage.